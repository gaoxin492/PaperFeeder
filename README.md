# ğŸ“š PaperFeeder

> **AI Agent for Daily Paper Digest**  
> Hunt for "The Next Big Thing", despise incremental work.

An intelligent paper recommendation system that automatically fetches, filters, researches, and summarizes academic papers from arXiv and HuggingFace. Powered by LLM agents and community signal enrichment.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## âœ¨ Key Features

### ğŸ¤– **AI Agent Workflow**
Six-stage intelligent pipeline that mimics how a senior researcher screens papers:

```
Fetch â†’ Keyword Filter â†’ LLM Coarse Filter â†’ Research & Enrichment â†’ LLM Fine Filter â†’ Synthesis
 (Recall)    (Cast Wide Net)    (Quick Score)      (Community Signals)       (Deep Ranking)     (Report)
```

### ğŸ” **Community Signal Enrichment**
- Uses **Tavily API** to search GitHub, Reddit, Twitter, HuggingFace
- Extracts: GitHub stars, community discussions, reproducibility issues
- Integrates external validation into paper evaluation

### ğŸ¯ **Two-Stage LLM Filtering**
- **Stage 1 (Coarse)**: Fast screening based on title + abstract â†’ Top 20
- **Stage 2 (Fine)**: Deep ranking with community signals â†’ Top 3-5

### ğŸ“° **"Editor's Choice" Style Reports**
- Senior Principal Researcher persona (OpenAI/DeepMind/Anthropic caliber)
- çŠ€åˆ©ç‚¹è¯„ï¼Œä¸­è‹±æ–‡å¤¹æ‚ (Sharp commentary, bilingual)
- Sections: ğŸ† Editor's Choice, ğŸ”¬ Deep Dive, ğŸŒ€ Signals & Noise

### ğŸ”§ **Flexible & Extensible**
- Supports any OpenAI-compatible LLM (OpenAI, Claude, Gemini, DeepSeek, Qwen, local models)
- PDF multimodal input for deep analysis (Claude, Gemini)
- Customizable research interests and filtering criteria

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- API Keys:
  - **Required**: LLM API key (OpenAI, Claude, etc.)
  - **Optional**: Tavily API key (for community research), Resend API key (for email)

### Installation

```bash
# Clone the repository
git clone https://github.com/gaoxin492/PaperFeeder.git
cd PaperFeeder

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Edit .env with your API keys
```

### Configuration

Create a `.env` file:

```bash
# LLM Settings (Main - for summarization)
LLM_API_KEY=sk-xxxxx
LLM_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-4o-mini

# LLM Settings (Filter - for two-stage filtering)
LLM_FILTER_API_KEY=sk-xxxxx  # Can use cheaper model
LLM_FILTER_BASE_URL=https://api.openai.com/v1
LLM_FILTER_MODEL=gpt-4o-mini

# Research & Enrichment (Optional)
TAVILY_API_KEY=tvly-xxxxx  # Get from https://tavily.com

# Email Delivery (Optional)
RESEND_API_KEY=re-xxxxx
EMAIL_FROM=papers@resend.dev
EMAIL_TO=your@email.com
```

### Run Locally

```bash
# Dry run (save report to HTML file)
python main.py --dry-run

# Send via email
python main.py

# Fetch last 3 days
python main.py --days 3
```

### ğŸ“§ Automated Daily Delivery

**Want daily paper digests delivered to your inbox automatically?**

Use **GitHub Actions** for **FREE** automated deployment (no server needed):

1. Fork this repository
2. Add your API keys as GitHub Secrets
3. Enable GitHub Actions
4. Receive daily emails at 8 AM (configurable)

**ğŸ‘‰ See [DEPLOY.md](DEPLOY.md) for complete setup guide** (takes ~5 minutes)

âœ¨ **Recommended**: Start with `--dry-run` locally to test your configuration, then deploy to GitHub Actions for daily automation!

---

## ğŸ—ï¸ Architecture

### AI Agent Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: FETCH (Recall)                                      â”‚
â”‚ â€¢ arXiv (cs.LG, cs.CL, cs.CV, etc.)                         â”‚
â”‚ â€¢ HuggingFace Daily Papers                                   â”‚
â”‚ â€¢ Manual additions                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ ~50-100 papers
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 2: KEYWORD FILTER (Cast Wide Net)                     â”‚
â”‚ â€¢ Match keywords in title + abstract                        â”‚
â”‚ â€¢ Exclude noise (medical, hardware, etc.)                   â”‚
â”‚ â€¢ Strategy: å®å¯é”™æ€ï¼Œä¸å¯æ¼è¿‡                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ ~30-50 papers
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 3: LLM COARSE FILTER (Quick Score)                    â”‚
â”‚ â€¢ Input: Title + Abstract + Authors                         â”‚
â”‚ â€¢ Criteria: Relevance, Novelty, Clarity                     â”‚
â”‚ â€¢ Output: Scores (0-10), Top 20 candidates                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ ~20 papers
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 4: RESEARCH & ENRICHMENT (Community Signals)          â”‚
â”‚ â€¢ Tavily search: GitHub, Reddit, Twitter, HuggingFace       â”‚
â”‚ â€¢ Extract: Stars, discussions, reproducibility              â”‚
â”‚ â€¢ Store in paper.research_notes                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ 20 papers (enriched)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 5: LLM FINE FILTER (Deep Ranking)                     â”‚
â”‚ â€¢ Input: Title + Abstract + Authors + Community Signals     â”‚
â”‚ â€¢ Criteria: Surprise, Significance, External Validation     â”‚
â”‚ â€¢ Output: Top 3-5 papers with detailed reasons              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ 3-5 papers
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 6: SYNTHESIS (Report Generation)                      â”‚
â”‚ â€¢ Senior Principal Researcher persona                       â”‚
â”‚ â€¢ PDF multimodal input (if supported)                       â”‚
â”‚ â€¢ Output: HTML report with MathJax support                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Module Overview

```
PaperFeeder/
â”œâ”€â”€ main.py              # AI Agent orchestration
â”œâ”€â”€ sources.py           # Paper fetchers (arXiv, HuggingFace, Manual)
â”œâ”€â”€ filters.py           # Two-stage LLM filtering
â”œâ”€â”€ researcher.py        # Tavily-powered community research (NEW)
â”œâ”€â”€ summarizer.py        # Report generation with community signals
â”œâ”€â”€ llm_client.py        # Universal LLM client (OpenAI-compatible)
â”œâ”€â”€ emailer.py           # Email delivery (Resend, SendGrid, File)
â”œâ”€â”€ models.py            # Data models (Paper, Author, etc.)
â”œâ”€â”€ config.py            # Configuration management
â””â”€â”€ config.yaml          # User configuration
```

---

## ğŸ“– Usage Guide

### Customize Research Interests

Edit `config.yaml`:

```yaml
research_interests: |
  You are a Senior Principal Researcher at a top-tier AI lab.
  
  ## What You're Hunting For
  1. Paradigm Shifts: Papers that challenge existing dogmas
  2. First-Principles Elegance: Strong mathematical foundations
  3. Scaling Insights: What actually works at scale
  
  ## Specific Technical Interests
  - Generative Models: Diffusion, Flow Matching, Autoregressive
  - Reasoning & System 2: CoT, Latent Reasoning, Test-time Compute
  - Representation Learning: JEPA, Contrastive Learning
  - AI Safety & Alignment: Interpretability, Scalable Oversight
  
  ## What You DESPISE
  - Incremental SOTA chasing
  - Prompt engineering as research
  - Pure benchmarks without insights
```

### Configure Keywords

```yaml
keywords:
  # Tier 1: Precision strikes
  - diffusion language model
  - test-time compute
  - mechanistic interpretability
  
  # Tier 2: Wide net (pair with exclude_keywords)
  - LLM
  - scaling law
  - foundation model

exclude_keywords:
  - medical
  - biomedical
  - 3D
  - video generation
```

### Use Different LLMs

```bash
# OpenAI
export LLM_BASE_URL=https://api.openai.com/v1
export LLM_MODEL=gpt-4o

# Claude (via Anthropic API)
export LLM_BASE_URL=https://api.anthropic.com/v1
export LLM_MODEL=claude-sonnet-4-20250514

# DeepSeek
export LLM_BASE_URL=https://api.deepseek.com/v1
export LLM_MODEL=deepseek-chat

# Gemini (via OpenAI-compatible endpoint)
export LLM_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai
export LLM_MODEL=gemini-2.0-flash-exp

# Local (Ollama)
export LLM_BASE_URL=http://localhost:11434/v1
export LLM_MODEL=llama3
```

### Cost Optimization

Use a **cheaper model for filtering**, stronger model for summarization:

```bash
# Filtering (called 2x per paper) - use cheap model
export LLM_FILTER_MODEL=gpt-4o-mini
export LLM_FILTER_BASE_URL=https://api.openai.com/v1

# Summarization (called once) - use better model
export LLM_MODEL=gpt-4o
export LLM_BASE_URL=https://api.openai.com/v1
```

---

## ğŸ¨ Report Example

### ğŸ† Editor's Choice

> **Diffusion Language Models Learn Latent Reasoning**
> 
> **Verdict**: è¿™æ˜¯æˆ‘ä»Šå¤©çœ‹åˆ°çš„å”¯ä¸€æœ‰"aha moment"çš„å·¥ä½œã€‚å°†discrete diffusionç”¨äºæ¨ç†ä»»åŠ¡ï¼Œè€Œä¸æ˜¯generationï¼Œè§†è§’æ–°é¢–ã€‚GitHubå·²è·800+ starsï¼ŒRedditä¸Šå…³äº"reasoning in latent space"çš„è®¨è®ºéå¸¸çƒ­çƒˆã€‚
> 
> **Signal**: GitHub repo with 823 stars. Active Reddit discussion on implications for o1-style reasoning. HuggingFace community highly engaged.

### ğŸ”¬ Deep Dive

**ğŸ‘¥ Authors**: Zhang et al. | Stanford, OpenAI

**ğŸ¯ The "Aha" Moment**: ä¼ ç»Ÿdiffusion modelsç”¨äºç”Ÿæˆï¼Œè¿™ç¯‡å°†å…¶ç”¨äºæ¨ç†ã€‚Core idea: reasoningæ˜¯ä¸€ä¸ªåœ¨latent spaceä¸­çš„iterative refinementè¿‡ç¨‹ï¼Œè€Œä¸æ˜¯token-by-tokençš„autoregressiveç”Ÿæˆã€‚ç¤¾åŒºåå“çƒ­çƒˆï¼Œè®¤ä¸ºè¿™å¯èƒ½æ˜¯post-CoTæ—¶ä»£çš„æ–°èŒƒå¼ã€‚

**ğŸ”§ Methodology**: ä½¿ç”¨continuous diffusionåœ¨embedding spaceæ“ä½œï¼Œè®­ç»ƒæ—¶å¼•å…¥"reasoning checkpoints"å¼ºåˆ¶æ¨¡å‹å­¦ä¼šåˆ†æ­¥æ¨ç†ã€‚å…³é”®trickæ˜¯å¼•å…¥äº†specialized noise schedule for logical consistencyã€‚

**ğŸ“Š Reality Check**: GSM8Kä¸Šè¾¾åˆ°89.2%ï¼ˆvs GPT-4çš„ 92%ï¼‰ï¼Œä½†åœ¨multi-hopæ¨ç†ä¸Šè¶…è¶Šäº†CoT baseline 12ä¸ªç‚¹ã€‚ç¤¾åŒºæŒ‡å‡ºä»£ç å¤ç°è¾ƒå®¹æ˜“ï¼Œå·²æœ‰3ä¸ªç‹¬ç«‹å¤ç°ã€‚

**ğŸ’¡ My Take**: å€¼å¾—è·Ÿè¿›ã€‚å¦‚æœscaling lawæˆç«‹ï¼Œè¿™å¯èƒ½æ˜¯reasoningçš„æ–°æ–¹å‘ã€‚å·²åŠ å…¥å¤ç°é˜Ÿåˆ—ã€‚

---

## ğŸ› ï¸ Advanced Features

### PDF Multimodal Input

For Claude and Gemini, full PDF is sent directly to the model:

```yaml
extract_fulltext: true
pdf_max_pages: 15  # Limit pages to save tokens
```

### Manual Paper Additions

Create `manual_papers.json`:

```json
{
  "papers": [
    {
      "title": "My Favorite Paper",
      "abstract": "...",
      "url": "https://arxiv.org/abs/2401.xxxxx",
      "notes": "Recommended by colleague"
    }
  ]
}
```

Or just add URLs (metadata auto-fetched):

```json
{
  "papers": [
    "https://arxiv.org/abs/2401.xxxxx",
    "https://arxiv.org/abs/2402.xxxxx"
  ]
}
```

### Disable Community Research

If you don't have Tavily API key:

```bash
# System will auto-detect and use mock researcher
unset TAVILY_API_KEY
python main.py --dry-run
```

---

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:

- [ ] Additional paper sources (Semantic Scholar, OpenReview)
- [ ] More research enrichment signals (citation counts, author h-index)
- [ ] Multi-language support
- [ ] Web UI / Chatbot integration
- [ ] Vector database for historical papers

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- Inspired by [Karpathy's blog](https://karpathy.github.io/) and the "senior researcher" mindset
- Built on top of [arXiv API](https://arxiv.org/help/api), [HuggingFace](https://huggingface.co/), and [Tavily](https://tavily.com/)
- Community feedback from AI research communities on Reddit and Twitter

---

## ğŸ“ Contact

- GitHub: [@gaoxin492](https://github.com/gaoxin492)
- Issues: [GitHub Issues](https://github.com/gaoxin492/PaperFeeder/issues)

---

**âš¡ Pro Tip**: Start with `--dry-run` to preview reports locally, then follow [DEPLOY.md](DEPLOY.md) to set up **free automated daily delivery** via GitHub Actions!

**ğŸ¯ Deployment**: For daily automated paper digests, see [DEPLOY.md](DEPLOY.md) for:
- ğŸ†“ **GitHub Actions** setup (recommended, no server needed)
- ğŸ–¥ï¸ Server deployment with cron jobs
- ğŸ³ Docker containerization

**Total setup time: ~5 minutes** â±ï¸