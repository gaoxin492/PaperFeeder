#!/usr/bin/env python3
"""
Daily Paper Assistant - AI Agent Workflow
Pipeline: Fetch â†’ Keyword Filter (Recall) â†’ LLM Coarse Filter â†’ Research (Enrichment) â†’ LLM Fine Filter (Ranking) â†’ Synthesize

NEW: Blog posts from priority sources (OpenAI, Anthropic, etc.) skip filtering!
"""

from __future__ import annotations

import asyncio
import argparse
import os
from datetime import datetime, timedelta
from typing import Optional, List

from sources import ArxivSource, HuggingFaceSource, ManualSource, BlogSource
from sources.blog_sources import fetch_blog_posts
from filters import KeywordFilter, LLMFilter
from researcher import PaperResearcher, MockPaperResearcher
from summarizer import PaperSummarizer
from emailer import ResendEmailer, FileEmailer
from config import Config
from models import Paper

# Check if blog sources are available (feedparser required)
try:
    import feedparser
    BLOG_SOURCE_AVAILABLE = True
except ImportError:
    BLOG_SOURCE_AVAILABLE = False



async def fetch_papers(config: Config, days_back: int = 1) -> List[Paper]:
    """Stage 1: Fetch papers from all sources (Recall)."""
    papers = []
    
    # arXiv
    print("ğŸ“š Fetching from arXiv...")
    arxiv_source = ArxivSource(config.arxiv_categories)
    arxiv_papers = await arxiv_source.fetch(days_back=days_back, max_results=300)
    papers.extend(arxiv_papers)
    print(f"   Found {len(arxiv_papers)} papers")
    
    # Hugging Face Daily Papers
    print("ğŸ¤— Fetching from HuggingFace Daily Papers...")
    hf_source = HuggingFaceSource()
    hf_papers = await hf_source.fetch()
    papers.extend(hf_papers)
    print(f"   Found {len(hf_papers)} papers")
    
    # Manual additions (from D1 or local)
    if config.manual_source_enabled:
        print("ğŸ“ Fetching manual additions...")
        manual_source = ManualSource(config.manual_source_path)
        manual_papers = await manual_source.fetch()
        papers.extend(manual_papers)
        print(f"   Found {len(manual_papers)} papers")
    
    # Deduplicate by arxiv_id or url
    seen = set()
    unique_papers = []
    for p in papers:
        key = p.arxiv_id or p.url
        if key not in seen:
            seen.add(key)
            unique_papers.append(p)
    
    print(f"âœ… Total unique papers: {len(unique_papers)}")
    return unique_papers


async def fetch_blogs(config: Config, days_back: int = 7) -> tuple[List[Paper], List[Paper]]:
    """
    Fetch blog posts from RSS feeds.
    
    Returns:
        (priority_posts, normal_posts)
        - priority_posts: From top labs (OpenAI, Anthropic, etc.), skip filtering
        - normal_posts: From other sources, go through normal pipeline
    """
    if not BLOG_SOURCE_AVAILABLE:
        return [], []
    
    # Check if blogs are enabled
    if not getattr(config, 'blogs_enabled', True):
        return [], []
    
    print("ğŸ“ Fetching from blogs...")
    
    # Get config options
    enabled_blogs = getattr(config, 'enabled_blogs', None)
    custom_blogs = getattr(config, 'custom_blogs', None)
    blog_days_back = getattr(config, 'blog_days_back', days_back)
    
    source = BlogSource(
        enabled_blogs=enabled_blogs,
        custom_blogs=custom_blogs,
        include_non_priority=True,
    )
    
    all_posts = await source.fetch(
        days_back=blog_days_back,
        max_posts_per_blog=5,
    )
    
    # Separate priority and normal
    priority_posts = [p for p in all_posts if getattr(p, 'skip_filter', False)]
    normal_posts = [p for p in all_posts if not getattr(p, 'skip_filter', False)]
    
    print(f"   ğŸ”¥ Priority blogs (skip filter): {len(priority_posts)}")
    print(f"   ğŸ“„ Normal blogs (go through filter): {len(normal_posts)}")
    
    return priority_posts, normal_posts


async def filter_papers_coarse(papers: List[Paper], config: Config) -> List[Paper]:
    """
    Stage 2 & 3: Apply filters to select relevant papers.
    
    Stage 2: Keyword filter (Recall) - ä¿ç•™è¾ƒå¤šæ•°é‡ï¼Œé¿å…æ¼ç½‘ä¹‹é±¼
    Stage 3: LLM Coarse filter - åŸºäºtitle+abstractç²—ç­›ï¼Œå¾—åˆ°Top 20
    """
    print(f"\nğŸ” Filtering {len(papers)} papers...")
    
    # Stage 2: Keyword filter (Recall)
    print("\n--- Stage 2: Keyword Filter (Recall) ---")
    keyword_filter = KeywordFilter(
        keywords=config.keywords,
        exclude_keywords=config.exclude_keywords
    )
    filtered = keyword_filter.filter(papers)
    print(f"   âœ… Keyword filter: {len(filtered)} papers matched (ä¿ç•™è¾ƒå¤šï¼Œé¿å…æ¼ç½‘)")
    
    # Stage 3: LLM Coarse filter (Title + Abstract only)
    print("\n--- Stage 3: LLM Coarse Filter (Title + Abstract) ---")
    if config.llm_filter_enabled and len(filtered) > config.llm_filter_threshold:
        # Use filter-specific API key if provided, otherwise use main LLM API key
        filter_api_key = config.llm_filter_api_key
        filter_base_url = config.llm_filter_base_url
        filter_model = config.llm_filter_model
        
        print(f"   ğŸ¤– Applying LLM Coarse Filter ({filter_model})...")
        llm_filter = LLMFilter(
            api_key=filter_api_key,
            research_interests=config.research_interests,
            base_url=filter_base_url,
            model=filter_model
        )
        
        # Coarse filtering: ä¸åŒ…å«community signals
        filtered = await llm_filter.filter(
            filtered, 
            max_papers=20,  # ç²—ç­›å¾—åˆ°Top 20è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
            include_community_signals=False
        )
        print(f"   âœ… LLM Coarse Filter: {len(filtered)} papers selected for enrichment")
        
        # Show top papers
        if filtered:
            print(f"   ğŸ“Œ Top candidates for research:")
            for i, paper in enumerate(filtered[:5], 1):
                score = getattr(paper, 'relevance_score', 0) * 10
                print(f"      {i}. [{score:.1f}/10] {paper.title[:60]}...")
    elif config.llm_filter_enabled:
        print(f"   â­• Skipping LLM Coarse Filter (only {len(filtered)} papers, threshold: {config.llm_filter_threshold})")
    
    return filtered


async def enrich_papers(papers: List[Paper], config: Config) -> List[Paper]:
    """
    Stage 4: Research (Enrichment) - è”ç½‘è°ƒç ”ï¼Œæ”¶é›†ç¤¾åŒºä¿¡å·
    """
    print("\n--- Stage 4: Research & Enrichment ---")
    
    # Check if Tavily API key is available
    tavily_api_key = config.tavily_api_key
    
    if not tavily_api_key:
        print("   âš ï¸  TAVILY_API_KEY not found, using mock researcher")
        researcher = MockPaperResearcher()
    else:
        print(f"   ğŸ” Using Tavily API for research")
        researcher = PaperResearcher(
            api_key=tavily_api_key,
            max_concurrent=5,
            search_depth="basic"
        )
    
    # Enrich papers with community signals
    enriched_papers = await researcher.research(papers)
    
    # Show some research results
    print(f"\n   ğŸ“Š Sample research notes:")
    for i, paper in enumerate(enriched_papers[:3], 1):
        notes = getattr(paper, 'research_notes', 'N/A')
        print(f"      {i}. {paper.title[:50]}...")
        print(f"         ğŸ” {notes[:100]}...")
    
    return enriched_papers


async def filter_papers_fine(papers: List[Paper], config: Config) -> List[Paper]:
    """
    Stage 5: LLM Fine Filter (Ranking) - åŸºäºcontent + community signalsç²¾ç­›
    ä»20ç¯‡ä¸­é€‰å‡ºçœŸæ­£å€¼å¾—æ·±åº¦é˜…è¯»çš„Top 3
    """
    print("\n--- Stage 5: LLM Fine Filter (Ranking with Community Signals) ---")
    
    if not config.llm_filter_enabled:
        print("   LLM filter disabled, returning all papers")
        return papers[:config.max_papers]
    
    filter_api_key = config.llm_filter_api_key or config.llm_api_key
    filter_base_url = config.llm_filter_base_url
    filter_model = config.llm_filter_model
    
    print(f"   ğŸ¤– Applying LLM Fine Filter with Community Signals ({filter_model})...")
    llm_filter = LLMFilter(
        api_key=filter_api_key,
        research_interests=config.research_interests,
        base_url=filter_base_url,
        model=filter_model
    )
    
    # Fine filtering: åŒ…å«community signals
    final_papers = await llm_filter.filter(
        papers,
        max_papers=config.max_papers,  # ç²¾ç­›å¾—åˆ°æœ€ç»ˆçš„Top 3
        include_community_signals=True  # å…³é”®: ä½¿ç”¨community signals
    )
    print(f"   âœ… LLM Fine Filter: Selected {len(final_papers)} papers for final report")
    
    # Show final selections with reasons
    if final_papers and hasattr(final_papers[0], 'filter_reason'):
        print(f"\n   ğŸ† Final selections:")
        for i, paper in enumerate(final_papers, 1):
            reason = getattr(paper, 'filter_reason', '')
            score = getattr(paper, 'relevance_score', 0) * 10
            print(f"      {i}. [{score:.1f}/10] {paper.title[:50]}...")
            if reason:
                print(f"         â†’ {reason[:80]}...")
    
    return final_papers


async def summarize_papers(papers: list[Paper], config: Config, priority_blogs: list[Paper] = None) -> str:
    """
    Stage 6: Synthesize - ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š

    Args:
        papers: Filtered papers from the pipeline
        config: Configuration
        priority_blogs: All blog posts (skip filtering/research, go directly to summarize)
    """
    print(f"\n--- Stage 6: Synthesis (Report Generation) ---")
    
    # Merge priority blogs with filtered papers
    all_content = []
    
    if priority_blogs:
        print(f"   ğŸ”¥ Including {len(priority_blogs)} priority blog posts")
        all_content.extend(priority_blogs)
    
    all_content.extend(papers)
    
    print(f"   ğŸ“ Generating report for {len(all_content)} items...")
    print(f"      - Blog posts: {len(priority_blogs) if priority_blogs else 0}")
    print(f"      - Papers: {len(papers)}")
    print(f"   Using: {config.llm_model} @ {config.llm_base_url}")
    
    # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®è¯»å–è°ƒè¯•é€‰é¡¹
    debug_save_pdfs = getattr(config, 'debug_save_pdfs', False)
    debug_pdf_dir = getattr(config, 'debug_pdf_dir', 'debug_pdfs')
    pdf_max_pages = getattr(config, 'pdf_max_pages', 10)
    
    summarizer = PaperSummarizer(
        api_key=config.llm_api_key,
        base_url=config.llm_base_url,
        model=config.llm_model,
        research_interests=config.research_interests,
        debug_save_pdfs=debug_save_pdfs,
        debug_pdf_dir=debug_pdf_dir,
        pdf_max_pages=pdf_max_pages
    )
    
    # ä½¿ç”¨PDFå¤šæ¨¡æ€è¾“å…¥ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
    report = await summarizer.generate_report(
        all_content,
        use_pdf_multimodal=config.extract_fulltext,
    )
    print("   âœ… Report generated!")
    return report


async def send_email(report: str, config: Config) -> bool:
    """Send the report via email."""
    print(f"\nğŸ“§ Sending email to {config.email_to}...")
    
    emailer = ResendEmailer(
        api_key=config.resend_api_key,
        from_email=config.email_from
    )
    
    today = datetime.now().strftime("%Y-%m-%d")
    subject = f"ğŸ“š Daily Paper Digest - {today}"
    
    success = await emailer.send(
        to=config.email_to,
        subject=subject,
        html_content=report
    )
    
    if success:
        print("   âœ… Email sent successfully!")
    else:
        print("   âŒ Failed to send email")
    
    return success


async def run_pipeline(config_path: str = "config.yaml", days_back: int = 1, dry_run: bool = False, no_papers: bool = False, no_blogs: bool = False):
    """
    Run the full AI Agent pipeline.

    Workflow:
    1. Fetch: è·å–è®ºæ–‡ (arXiv, HuggingFace, Manual) - å¯é€‰
    1b. Fetch Blogs: è·å–åšå®¢ (å®Œå…¨è·³è¿‡è¿‡æ»¤/ç ”ç©¶ï¼Œç›´æ¥è¿›summarize) - å¯é€‰
    2. Keyword Filter (Recall): å…³é”®è¯åŒ¹é…ï¼Œä¿ç•™è¾ƒå¤šæ•°é‡ (ä»…è®ºæ–‡)
    3. LLM Coarse Filter: åŸºäºtitle+abstractç²—ç­›ï¼Œå¾—åˆ°Top 20 (ä»…è®ºæ–‡)
    4. Research (Enrichment): è”ç½‘è°ƒç ”Top 20ï¼Œè·å–ç¤¾åŒºä¿¡å· (ä»…è®ºæ–‡)
    5. LLM Fine Filter (Ranking): åŸºäºcontent+signalsç²¾ç­›ï¼Œå¾—åˆ°Top 3 (ä»…è®ºæ–‡)
    6. Synthesize: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š (è®ºæ–‡ + æ‰€æœ‰åšå®¢)

    Use --no-papers or --no-blogs to selectively disable sources.
    Blogs skip all filtering/research stages and go directly to report generation.
    """
    print("=" * 80)
    print(f"ğŸš€ PaperFeeder AI Agent - {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print("=" * 80)
    print("\nğŸ“‹ Workflow: Fetch â†’ Recall â†’ Coarse Filter â†’ Enrich â†’ Fine Filter â†’ Synthesize\n")

    # Set environment variables for source control before loading config
    if no_papers:
        os.environ['PAPERS_ENABLED'] = 'false'
    if no_blogs:
        os.environ['BLOGS_ENABLED'] = 'false'

    # Load config
    config = Config.from_yaml(config_path)
    
    # Stage 1: Fetch (Recall)
    print("=" * 80)
    print("STAGE 1: FETCH (Recall)")
    print("=" * 80)

    # Fetch papers if enabled
    papers = []
    if getattr(config, 'papers_enabled', True):
        papers = await fetch_papers(config, days_back=days_back)
    else:
        print("   ğŸ“„ Paper fetching disabled (--no-papers flag)")

    # Stage 1b: Fetch Blogs (directly to summarize, skip filtering/research)
    priority_blogs, normal_blogs = await fetch_blogs(config, days_back=7)

    # Combine all blogs for direct summarization
    all_blogs = priority_blogs + normal_blogs
    if all_blogs:
        print(f"   ğŸ“ {len(all_blogs)} blogs fetched (skip filtering, go directly to summarize)")

    if not papers and not all_blogs:
        print("\nâš ï¸ No papers or blogs found. Exiting.")
        return

    # Stage 2-3: Keyword Filter + LLM Coarse Filter (papers only)
    coarse_filtered = []
    if papers:
        print("\n" + "=" * 80)
        print("STAGE 2-3: FILTERING (Recall â†’ Coarse)")
        print("=" * 80)
        coarse_filtered = await filter_papers_coarse(papers, config)

    if not coarse_filtered and not all_blogs:
        print("\nâš ï¸ No papers passed coarse filter and no blogs. Exiting.")
        return

    # Stage 4: Research & Enrichment (papers only)
    enriched_papers = []
    if coarse_filtered:
        print("\n" + "=" * 80)
        print("STAGE 4: ENRICHMENT (Research)")
        print("=" * 80)
        enriched_papers = await enrich_papers(coarse_filtered, config)

    # Stage 5: LLM Fine Filter (Ranking) (papers only)
    final_papers = []
    if enriched_papers:
        print("\n" + "=" * 80)
        print("STAGE 5: RANKING (Fine Filter with Signals)")
        print("=" * 80)
        final_papers = await filter_papers_fine(enriched_papers, config)
    
    if not final_papers and not all_blogs:
        print("\nâš ï¸ No papers passed fine filter and no blogs. Exiting.")
        return
    
    # Stage 6: Synthesize (includes all blogs!)
    print("\n" + "=" * 80)
    print("STAGE 6: SYNTHESIS (Report Generation)")
    print("=" * 80)
    report = await summarize_papers(final_papers, config, priority_blogs=all_blogs)
    
    # Output/Send
    print("\n" + "=" * 80)
    print("DELIVERY")
    print("=" * 80)
    
    if dry_run:
        print("\nğŸ“ DRY RUN - Saving report to file...")
        file_emailer = FileEmailer("report_preview.html")
        await file_emailer.send(
            to=config.email_to,
            subject=f"Paper Digest - {datetime.now().strftime('%Y-%m-%d')}",
            html_content=report
        )
        print("âœ… Report saved to report_preview.html")
    else:
        await send_email(report, config)
    
    print("\n" + "=" * 80)
    print("âœ¨ Pipeline Complete!")
    print("=" * 80)
    print(f"\nğŸ“Š Summary:")
    print(f"   - Papers fetched: {len(papers)}")
    print(f"   - Blogs fetched (skip filter/research): {len(all_blogs)}")
    print(f"   - After keyword filter: {len(coarse_filtered) if coarse_filtered else 0}")
    print(f"   - After enrichment: {len(enriched_papers) if enriched_papers else 0}")
    print(f"   - Final papers: {len(final_papers) if final_papers else 0}")
    print(f"   - Total in report: {len(final_papers) + len(all_blogs)}")


def main():
    parser = argparse.ArgumentParser(
        description="PaperFeeder AI Agent - Hunt for 'The Next Big Thing'",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Workflow:
  1. Fetch papers from arXiv, HuggingFace, and manual sources (optional)
  1b. Fetch blogs from RSS feeds (optional, priority blogs skip filtering!)
  2. Keyword filter (Recall) - Cast a wide net
  3. LLM Coarse filter - Quick scoring based on title/abstract â†’ Top 20
  4. Research & Enrichment - Gather community signals via Tavily API
  5. LLM Fine filter - Deep ranking with community signals â†’ Top 3
  6. Synthesis - Generate "Editor's Choice" style report

NEW: Flexible Source Selection
  - Use --no-papers to disable paper fetching (only blogs)
  - Use --no-blogs to disable blog fetching (only papers)
  - Priority blogs (OpenAI, Anthropic, DeepMind, etc.) skip filtering
  - Normal blogs go through the full pipeline
  - Configure in config.yaml or via environment variables

Environment Variables:
  LLM_API_KEY         - Main LLM API key (for summarization)
  LLM_FILTER_API_KEY  - Filter LLM API key (optional, uses cheaper model)
  TAVILY_API_KEY      - Tavily search API key (for research stage)
  RESEND_API_KEY      - Email delivery API key
  EMAIL_TO            - Recipient email address
        """
    )
    parser.add_argument("--config", default="config.yaml", help="Path to config file")
    parser.add_argument("--days", type=int, default=1, help="Days to look back for papers")
    parser.add_argument("--blog-days", type=int, default=7, help="Days to look back for blogs")
    parser.add_argument("--dry-run", action="store_true", help="Don't send email, save to file")
    parser.add_argument("--no-blogs", action="store_true", help="Disable blog fetching")
    parser.add_argument("--no-papers", action="store_true", help="Disable paper fetching")
    
    args = parser.parse_args()
    
    asyncio.run(run_pipeline(
        config_path=args.config,
        days_back=args.days,
        dry_run=args.dry_run,
        no_papers=args.no_papers,
        no_blogs=args.no_blogs
    ))


if __name__ == "__main__":
    main()