#!/usr/bin/env python3
"""
Daily Paper Assistant - AI Agent Workflow
Pipeline: Fetch â†’ Keyword Filter (Recall) â†’ LLM Coarse Filter â†’ Research (Enrichment) â†’ LLM Fine Filter (Ranking) â†’ Synthesize
"""

from __future__ import annotations

import asyncio
import argparse
import os
from datetime import datetime, timedelta
from typing import Optional, List

from sources import ArxivSource, HuggingFaceSource, ManualSource
from filters import KeywordFilter, LLMFilter
from researcher import PaperResearcher, MockPaperResearcher
from summarizer import PaperSummarizer
from emailer import ResendEmailer, FileEmailer
from config import Config
from models import Paper


async def fetch_papers(config: Config, days_back: int = 1) -> List[Paper]:
    """Stage 1: Fetch papers from all sources (Recall)."""
    papers = []
    
    # arXiv
    print("ğŸ“š Fetching from arXiv...")
    arxiv_source = ArxivSource(config.arxiv_categories)
    arxiv_papers = await arxiv_source.fetch(days_back=days_back, max_results=300)
    papers.extend(arxiv_papers)
    print(f"   Found {len(arxiv_papers)} papers")
    
    # Hugging Face Daily Papers
    print("ğŸ¤— Fetching from HuggingFace Daily Papers...")
    hf_source = HuggingFaceSource()
    hf_papers = await hf_source.fetch()
    papers.extend(hf_papers)
    print(f"   Found {len(hf_papers)} papers")
    
    # Manual additions (from D1 or local)
    if config.manual_source_enabled:
        print("ğŸ“ Fetching manual additions...")
        manual_source = ManualSource(config.manual_source_path)
        manual_papers = await manual_source.fetch()
        papers.extend(manual_papers)
        print(f"   Found {len(manual_papers)} papers")
    
    # Deduplicate by arxiv_id or url
    seen = set()
    unique_papers = []
    for p in papers:
        key = p.arxiv_id or p.url
        if key not in seen:
            seen.add(key)
            unique_papers.append(p)
    
    print(f"âœ… Total unique papers: {len(unique_papers)}")
    return unique_papers


async def filter_papers_coarse(papers: List[Paper], config: Config) -> List[Paper]:
    """
    Stage 2 & 3: Apply filters to select relevant papers.
    
    Stage 2: Keyword filter (Recall) - ä¿ç•™è¾ƒå¤šæ•°é‡ï¼Œé¿å…æ¼ç½‘ä¹‹é±¼
    Stage 3: LLM Coarse filter - åŸºäºtitle+abstractç²—ç­›ï¼Œå¾—åˆ°Top 20
    """
    print(f"\nğŸ” Filtering {len(papers)} papers...")
    
    # Stage 2: Keyword filter (Recall)
    print("\n--- Stage 2: Keyword Filter (Recall) ---")
    keyword_filter = KeywordFilter(
        keywords=config.keywords,
        exclude_keywords=config.exclude_keywords
    )
    filtered = keyword_filter.filter(papers)
    print(f"   âœ… Keyword filter: {len(filtered)} papers matched (ä¿ç•™è¾ƒå¤šï¼Œé¿å…æ¼ç½‘)")
    
    # Stage 3: LLM Coarse filter (Title + Abstract only)
    print("\n--- Stage 3: LLM Coarse Filter (Title + Abstract) ---")
    if config.llm_filter_enabled and len(filtered) > config.llm_filter_threshold:
        # Use filter-specific API key if provided, otherwise use main LLM API key
        filter_api_key = config.llm_filter_api_key
        filter_base_url = config.llm_filter_base_url
        filter_model = config.llm_filter_model
        
        print(f"   ğŸ¤– Applying LLM Coarse Filter ({filter_model})...")
        llm_filter = LLMFilter(
            api_key=filter_api_key,
            research_interests=config.research_interests,
            base_url=filter_base_url,
            model=filter_model
        )
        
        # Coarse filtering: ä¸åŒ…å«community signals
        filtered = await llm_filter.filter(
            filtered, 
            max_papers=20,  # ç²—ç­›å¾—åˆ°Top 20è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
            include_community_signals=False
        )
        print(f"   âœ… LLM Coarse Filter: {len(filtered)} papers selected for enrichment")
        
        # Show top papers
        if filtered:
            print(f"   ğŸ“Œ Top candidates for research:")
            for i, paper in enumerate(filtered[:5], 1):
                score = getattr(paper, 'relevance_score', 0) * 10
                print(f"      {i}. [{score:.1f}/10] {paper.title[:60]}...")
    elif config.llm_filter_enabled:
        print(f"   â­ï¸ Skipping LLM Coarse Filter (only {len(filtered)} papers, threshold: {config.llm_filter_threshold})")
    
    return filtered


async def enrich_papers(papers: List[Paper], config: Config) -> List[Paper]:
    """
    Stage 4: Research (Enrichment) - è”ç½‘è°ƒç ”ï¼Œæ”¶é›†ç¤¾åŒºä¿¡å·
    """
    print("\n--- Stage 4: Research & Enrichment ---")
    
    # Check if Tavily API key is available
    tavily_api_key = config.tavily_api_key
    
    if not tavily_api_key:
        print("   âš ï¸  TAVILY_API_KEY not found, using mock researcher")
        researcher = MockPaperResearcher()
    else:
        print(f"   ğŸ” Using Tavily API for research")
        researcher = PaperResearcher(
            api_key=tavily_api_key,
            max_concurrent=5,
            search_depth="basic"
        )
    
    # Enrich papers with community signals
    enriched_papers = await researcher.research(papers)
    
    # Show some research results
    print(f"\n   ğŸ“Š Sample research notes:")
    for i, paper in enumerate(enriched_papers[:3], 1):
        notes = getattr(paper, 'research_notes', 'N/A')
        print(f"      {i}. {paper.title[:50]}...")
        print(f"         ğŸ” {notes[:100]}...")
    
    return enriched_papers


async def filter_papers_fine(papers: List[Paper], config: Config) -> List[Paper]:
    """
    Stage 5: LLM Fine Filter (Ranking) - åŸºäºcontent + community signalsç²¾ç­›
    ä»20ç¯‡ä¸­é€‰å‡ºçœŸæ­£å€¼å¾—æ·±åº¦é˜…è¯»çš„Top 3
    """
    print("\n--- Stage 5: LLM Fine Filter (Ranking with Community Signals) ---")
    
    if not config.llm_filter_enabled:
        print("   LLM filter disabled, returning all papers")
        return papers[:config.max_papers]
    
    filter_api_key = config.llm_filter_api_key or config.llm_api_key
    filter_base_url = config.llm_filter_base_url
    filter_model = config.llm_filter_model
    
    print(f"   ğŸ¤– Applying LLM Fine Filter with Community Signals ({filter_model})...")
    llm_filter = LLMFilter(
        api_key=filter_api_key,
        research_interests=config.research_interests,
        base_url=filter_base_url,
        model=filter_model
    )
    
    # Fine filtering: åŒ…å«community signals
    final_papers = await llm_filter.filter(
        papers,
        max_papers=config.max_papers,  # ç²¾ç­›å¾—åˆ°æœ€ç»ˆçš„Top 3
        include_community_signals=True  # å…³é”®: ä½¿ç”¨community signals
    )
    print(f"   âœ… LLM Fine Filter: Selected {len(final_papers)} papers for final report")
    
    # Show final selections with reasons
    if final_papers and hasattr(final_papers[0], 'filter_reason'):
        print(f"\n   ğŸ† Final selections:")
        for i, paper in enumerate(final_papers, 1):
            reason = getattr(paper, 'filter_reason', '')
            score = getattr(paper, 'relevance_score', 0) * 10
            print(f"      {i}. [{score:.1f}/10] {paper.title[:50]}...")
            if reason:
                print(f"         â†’ {reason[:80]}...")
    
    return final_papers


async def summarize_papers(papers: list[Paper], config: Config) -> str:
    """Stage 6: Synthesize - ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    print(f"\n--- Stage 6: Synthesis (Report Generation) ---")
    print(f"   ğŸ“ Generating report for {len(papers)} papers...")
    print(f"   Using: {config.llm_model} @ {config.llm_base_url}")
    
    # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®è¯»å–è°ƒè¯•é€‰é¡¹
    debug_save_pdfs = getattr(config, 'debug_save_pdfs', False)
    debug_pdf_dir = getattr(config, 'debug_pdf_dir', 'debug_pdfs')
    pdf_max_pages = getattr(config, 'pdf_max_pages', 10)
    
    summarizer = PaperSummarizer(
        api_key=config.llm_api_key,
        base_url=config.llm_base_url,
        model=config.llm_model,
        research_interests=config.research_interests,
        debug_save_pdfs=debug_save_pdfs,
        debug_pdf_dir=debug_pdf_dir,
        pdf_max_pages=pdf_max_pages
    )
    
    # ä½¿ç”¨PDFå¤šæ¨¡æ€è¾“å…¥ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
    report = await summarizer.generate_report(
        papers,
        use_pdf_multimodal=config.extract_fulltext,
    )
    print("   âœ… Report generated!")
    return report


async def send_email(report: str, config: Config) -> bool:
    """Send the report via email."""
    print(f"\nğŸ“§ Sending email to {config.email_to}...")
    
    emailer = ResendEmailer(
        api_key=config.resend_api_key,
        from_email=config.email_from
    )
    
    today = datetime.now().strftime("%Y-%m-%d")
    subject = f"ğŸ“š Daily Paper Digest - {today}"
    
    success = await emailer.send(
        to=config.email_to,
        subject=subject,
        html_content=report
    )
    
    if success:
        print("   âœ… Email sent successfully!")
    else:
        print("   âŒ Failed to send email")
    
    return success


async def run_pipeline(config_path: str = "config.yaml", days_back: int = 1, dry_run: bool = False):
    """
    Run the full AI Agent pipeline.
    
    Workflow:
    1. Fetch: è·å–è®ºæ–‡ (arXiv, HuggingFace, Manual)
    2. Keyword Filter (Recall): å…³é”®è¯åŒ¹é…ï¼Œä¿ç•™è¾ƒå¤šæ•°é‡
    3. LLM Coarse Filter: åŸºäºtitle+abstractç²—ç­›ï¼Œå¾—åˆ°Top 20
    4. Research (Enrichment): è”ç½‘è°ƒç ”Top 20ï¼Œè·å–ç¤¾åŒºä¿¡å·
    5. LLM Fine Filter (Ranking): åŸºäºcontent+signalsç²¾ç­›ï¼Œå¾—åˆ°Top 3
    6. Synthesize: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    """
    print("=" * 80)
    print(f"ğŸš€ PaperFeeder AI Agent - {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print("=" * 80)
    print("\nğŸ“‹ Workflow: Fetch â†’ Recall â†’ Coarse Filter â†’ Enrich â†’ Fine Filter â†’ Synthesize\n")
    
    # Load config
    config = Config.from_yaml(config_path)
    
    # Stage 1: Fetch (Recall)
    print("=" * 80)
    print("STAGE 1: FETCH (Recall)")
    print("=" * 80)
    papers = await fetch_papers(config, days_back=days_back)
    
    if not papers:
        print("\nâš ï¸ No papers found. Exiting.")
        return
    
    # Stage 2-3: Keyword Filter + LLM Coarse Filter
    print("\n" + "=" * 80)
    print("STAGE 2-3: FILTERING (Recall â†’ Coarse)")
    print("=" * 80)
    coarse_filtered = await filter_papers_coarse(papers, config)
    
    if not coarse_filtered:
        print("\nâš ï¸ No papers passed coarse filter. Exiting.")
        return
    
    # Stage 4: Research & Enrichment
    print("\n" + "=" * 80)
    print("STAGE 4: ENRICHMENT (Research)")
    print("=" * 80)
    enriched_papers = await enrich_papers(coarse_filtered, config)
    
    # Stage 5: LLM Fine Filter (Ranking)
    print("\n" + "=" * 80)
    print("STAGE 5: RANKING (Fine Filter with Signals)")
    print("=" * 80)
    final_papers = await filter_papers_fine(enriched_papers, config)
    
    if not final_papers:
        print("\nâš ï¸ No papers passed fine filter. Exiting.")
        return
    
    # Stage 6: Synthesize
    print("\n" + "=" * 80)
    print("STAGE 6: SYNTHESIS (Report Generation)")
    print("=" * 80)
    report = await summarize_papers(final_papers, config)
    
    # Output/Send
    print("\n" + "=" * 80)
    print("DELIVERY")
    print("=" * 80)
    
    if dry_run:
        print("\nğŸ” DRY RUN - Saving report to file...")
        file_emailer = FileEmailer("report_preview.html")
        await file_emailer.send(
            to=config.email_to,
            subject=f"Paper Digest - {datetime.now().strftime('%Y-%m-%d')}",
            html_content=report
        )
        print("âœ… Report saved to report_preview.html")
    else:
        await send_email(report, config)
    
    print("\n" + "=" * 80)
    print("âœ¨ Pipeline Complete!")
    print("=" * 80)
    print(f"\nğŸ“Š Summary:")
    print(f"   - Papers fetched: {len(papers)}")
    print(f"   - After keyword filter: {len(coarse_filtered)}")
    print(f"   - After enrichment: {len(enriched_papers)}")
    print(f"   - Final selection: {len(final_papers)}")


def main():
    parser = argparse.ArgumentParser(
        description="PaperFeeder AI Agent - Hunt for 'The Next Big Thing'",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Workflow:
  1. Fetch papers from arXiv, HuggingFace, and manual sources
  2. Keyword filter (Recall) - Cast a wide net
  3. LLM Coarse filter - Quick scoring based on title/abstract â†’ Top 20
  4. Research & Enrichment - Gather community signals via Tavily API
  5. LLM Fine filter - Deep ranking with community signals â†’ Top 3
  6. Synthesis - Generate "Editor's Choice" style report

Environment Variables:
  LLM_API_KEY         - Main LLM API key (for summarization)
  LLM_FILTER_API_KEY  - Filter LLM API key (optional, uses cheaper model)
  TAVILY_API_KEY      - Tavily search API key (for research stage)
  RESEND_API_KEY      - Email delivery API key
  EMAIL_TO            - Recipient email address
        """
    )
    parser.add_argument("--config", default="config.yaml", help="Path to config file")
    parser.add_argument("--days", type=int, default=1, help="Days to look back")
    parser.add_argument("--dry-run", action="store_true", help="Don't send email, save to file")
    
    args = parser.parse_args()
    
    asyncio.run(run_pipeline(
        config_path=args.config,
        days_back=args.days,
        dry_run=args.dry_run
    ))


if __name__ == "__main__":
    main()